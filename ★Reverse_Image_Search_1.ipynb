{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reverse Image Search Engine\n",
        "- Reverse Image Search is one of engine in the field of Computer vision I very interesting\n",
        "- This engine explore how one can use embeddings â€” a contextual representation of an image to find similar images\n",
        "- explore different strategies and algorithms to speed this up at scale, from thousands to several million images, and making them searchable in microseconds. "
      ],
      "metadata": {
        "id": "N0WyQxO6aO-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##content\n",
        "1.   **feature_extraction**: extract image to the feature from CNN\n",
        "2.   similarity-search: index features and search for most similar features using nearest neighbor algorithms, and visualizing plots\n",
        "3.   reduce-feature-length-with-pca: experiment with PCA and figure out what is the optimum length of the features to use"
      ],
      "metadata": {
        "id": "X2RkDr5xaVDH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Qg7ispWvO1"
      },
      "source": [
        "##1.feature_extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDXSQZ3o05yP",
        "outputId": "f5861a4c-85bf-42b6-92c4-a65efdf25d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvfrwuY1r_MQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpIgUXWQW3ck"
      },
      "source": [
        "Step1: import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryyANt7Ebimy",
        "outputId": "4acdbd15-93fd-485b-d56d-33dd2f2b20a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
            "/usr/local/lib/python2.7/dist-packages/gdown/parse_url.py:31: UserWarning: You specified Google Drive Link but it is not the correct link to download the file. Maybe you should try: https://drive.google.com/uc?id=None\n",
            "  .format(url='https://drive.google.com/uc?id={}'.format(file_id))\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n",
            "To: /datasets/caltech101.tar.gz\n",
            "63.5kB [00:00, 23.3MB/s]\n",
            "\n",
            "gzip: stdin: not in gzip format\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat '../../datasets/101_ObjectCategories': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ../../datasets\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp --output ../../datasets/caltech101.tar.gz\n",
        "!tar -xvzf ../../datasets/caltech101.tar.gz --directory ../../datasets\n",
        "!mv ../../datasets/101_ObjectCategories ../../datasets/caltech101\n",
        "!rm -rf ../../datasets/caltech101/BACKGROUND_Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eX7BuIQXDvM"
      },
      "source": [
        "Step2: Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgE1K96SnwoN"
      },
      "outputs": [],
      "source": [
        "def model_picker(name):\n",
        "    if (name == 'vgg16'):\n",
        "        model = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'vgg19'):\n",
        "        model = VGG19(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'mobilenet'):\n",
        "        model = MobileNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(224, 224, 3),\n",
        "                          pooling='max',\n",
        "                          depth_multiplier=1,\n",
        "                          alpha=1)\n",
        "    elif (name == 'inception'):\n",
        "        model = InceptionV3(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=(224, 224, 3),\n",
        "                            pooling='max')\n",
        "    elif (name == 'resnet'):\n",
        "        model = ResNet50(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                        pooling='max')\n",
        "    elif (name == 'xception'):\n",
        "        model = Xception(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                         pooling='max')\n",
        "    else:\n",
        "        print(\"Specified model not available\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAZrNjJzZAYt",
        "outputId": "e4fcc9db-4cc7-4c71-a87e-310b2954888a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model_architecture = 'resnet'\n",
        "model = model_picker(model_architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIzUCWN7XYvD"
      },
      "source": [
        "Step3: Create Extract feature (include step2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcE-0zm0nuI6"
      },
      "outputs": [],
      "source": [
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224, 3)\n",
        "    img = image.load_img(img_path,\n",
        "                         target_size=(input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAYlSk1fXeMF"
      },
      "source": [
        "Test A: Extract feature TEST with cat.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvYqXoswR6kJ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IS_COLAB_ENV = True\n",
        "except:\n",
        "  IS_COLAB_ENV = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqxir-zpmRPt",
        "outputId": "8d4a0c4e-c257-4e10-c8aa-8e14ba418a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  661k  100  661k    0     0  1407k      0 --:--:-- --:--:-- --:--:-- 1404k\n"
          ]
        }
      ],
      "source": [
        "IMG_PATH = '../../sample-images/cat.jpg'\n",
        "if IS_COLAB_ENV:\n",
        "  !curl https://raw.githubusercontent.com/PracticalDL/Practical-Deep-Learning-Book/master/sample-images/cat.jpg --output cat.jpg\n",
        "  IMG_PATH = 'cat.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW19ONeemT9F",
        "outputId": "b62c27d4-eddd-4e60-dd26-de8dfa60b756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total length of features for one image:  2048\n"
          ]
        }
      ],
      "source": [
        "features = extract_features('cat.jpg', model)\n",
        "print(\"Total length of features for one image: \", len(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWJTpirIXw8x"
      },
      "source": [
        "Step4: extract freture with REAL dataset and pickle dump\n",
        "\n",
        "*   File name -> tranfrom image name to text keep in the arry [,,]\n",
        "*   File list -> get the feature alredy extract keep in the arry [,,] \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGbDfduSi1wg"
      },
      "outputs": [],
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                if os.path.exists(filepath):\n",
        "                  file_list.append(filepath)\n",
        "                else:\n",
        "                  print(filepath)\n",
        "    return file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTmOZWXmVmr0",
        "outputId": "0f13aad6-bed7-4141-ae64-129bfae8c915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8677\n"
          ]
        }
      ],
      "source": [
        "# path to the your datasets\n",
        "root_dir = '/content/gdrive/MyDrive/image_search/101_ObjectCategories'\n",
        "filenames = sorted(get_file_list(root_dir))\n",
        "print(len(filenames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d8cea24807384a20bf42b870e93833f6",
            "a053d37dfc934616990a2652357cc005",
            "105e087b6d654baebef6417f7dd9fba7",
            "a3f59c1620154175a213041bfa5bb0a9",
            "57f8be57223442afae05614305d5cbae",
            "441f9287bb8243139c85f07b17093d58",
            "788612e6993b45339eb95788ef506987",
            "6e2c724cf627472987999eabedbd41f2",
            "e145bcb2745e40bf984cd53dd9d9879a",
            "098b09e1e2314c96961d49beb25600c3",
            "a420b14225d54cc7a5656fedf8446bd0"
          ]
        },
        "id": "JJ9N0h9YmAWT",
        "outputId": "50078aea-db5e-42bb-d2c2-b36f949e1785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8cea24807384a20bf42b870e93833f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8677 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "standard_feature_list = []\n",
        "for i in tqdm_notebook(range(len(filenames))):\n",
        "    standard_feature_list.append(extract_features(filenames[i], model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0uYrx_zuH_d"
      },
      "source": [
        "Now let's try the same with the Keras Image Generator functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aSAsaNglNK-",
        "outputId": "d681f4a2-9b35-490a-8617-aca0882b043c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "generator = datagen.flow_from_directory(root_dir,\n",
        "                                        target_size=(224, 224),\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False)\n",
        "\n",
        "num_images = len(generator.filenames)\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\n",
        "\n",
        "start_time = time.time()\n",
        "feature_list = []\n",
        "feature_list = model.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MCXb3ffWU8_z",
        "outputId": "48a720f3-10ef-41cd-f699-3d17a2446029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/image_search/101_ObjectCategories'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6tD0RLSnvbS",
        "outputId": "23c03eba-55bd-4b75-a48b-9ec4d67b5e13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(feature_list[0]) ## continue using OOOO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF800wG_tW6N",
        "outputId": "33a1cf71-e41c-4870-f08c-d4f5e83dbf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num images   =  8677\n",
            "Shape of feature_list =  (2176, 101)\n",
            "Time taken in sec =  309.36880373954773\n"
          ]
        }
      ],
      "source": [
        "for i, features in enumerate(feature_list):\n",
        "    feature_list[i] = features / norm(features)\n",
        "\n",
        "feature_list = feature_list.reshape(len(feature_list), -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBkT5jhxUgkH",
        "outputId": "ee5caf5a-c8a8-4929-fe76-01d26e24695d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.7593457e-01, 2.1806362e-01, 4.2282813e-09, ..., 5.4905178e-12,\n",
              "        5.9562160e-09, 1.9910151e-07],\n",
              "       [9.9999505e-01, 3.1596171e-03, 3.1561712e-11, ..., 1.5960328e-14,\n",
              "        6.0038856e-11, 4.9669979e-09],\n",
              "       [8.9359927e-01, 4.4886565e-01, 1.1590574e-08, ..., 1.5354574e-12,\n",
              "        1.2531770e-09, 2.6928122e-09],\n",
              "       ...,\n",
              "       [7.5723401e-09, 3.3564134e-09, 1.0619596e-06, ..., 8.4453734e-07,\n",
              "        6.0345948e-05, 2.9422119e-06],\n",
              "       [3.2655710e-12, 2.6009330e-13, 1.6824890e-12, ..., 4.2382201e-10,\n",
              "        2.3043820e-06, 3.5522481e-09],\n",
              "       [7.3336793e-07, 1.0855513e-06, 3.2148044e-05, ..., 1.7060293e-06,\n",
              "        7.7939819e-04, 2.6305843e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrjtC_umtV0L"
      },
      "outputs": [],
      "source": [
        "filenames = [root_dir + '/' + s for s in generator.filenames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgv4dVgqfcua"
      },
      "outputs": [],
      "source": [
        "pickle.dump(generator.classes, open('/content/gdrive/MyDrive/Reverse_Image_Search/class_ids-caltech101.pickle','wb'))\n",
        "pickle.dump(filenames, open('/content/gdrive/MyDrive/Reverse_Image_Search/filenames-caltech101.pickle', 'wb'))\n",
        "pickle.dump(feature_list,open('/content/gdrive/MyDrive/Reverse_Image_Search/features-caltech101-' + model_architecture + '.pickle', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dJ7qoNFkw6W"
      },
      "source": [
        "Step 5: Let's train the finetuned model and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc4mVsAFfcX4"
      },
      "outputs": [],
      "source": [
        "TRAIN_SAMPLES = 8677\n",
        "NUM_CLASSES = 101\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV-fQ9WrejsC"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   zoom_range=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac63YM62k4Ln",
        "outputId": "9dfb4a57-5e71-49c4-8069-ff69f7135a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=12345,\n",
        "                                                    class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMrcdLp7k4Ip"
      },
      "outputs": [],
      "source": [
        "def model_maker():\n",
        "    base_model = ResNet50(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIC7osH3k4Fm",
        "outputId": "88f49051-c4e6-443f-acce-f80df043c5f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "68/68 [==============================] - 496s 7s/step - loss: 3.5973 - acc: 0.2564\n",
            "Epoch 2/10\n",
            "68/68 [==============================] - 399s 6s/step - loss: 2.7672 - acc: 0.3824\n",
            "Epoch 3/10\n",
            "68/68 [==============================] - 356s 5s/step - loss: 2.2610 - acc: 0.4706\n",
            "Epoch 4/10\n",
            "68/68 [==============================] - 338s 5s/step - loss: 2.0571 - acc: 0.5077\n",
            "Epoch 5/10\n",
            "68/68 [==============================] - 336s 5s/step - loss: 1.8095 - acc: 0.5611\n",
            "Epoch 6/10\n",
            "68/68 [==============================] - 336s 5s/step - loss: 1.5415 - acc: 0.6117\n",
            "Epoch 7/10\n",
            "68/68 [==============================] - 331s 5s/step - loss: 1.3680 - acc: 0.6388\n",
            "Epoch 8/10\n",
            "68/68 [==============================] - 329s 5s/step - loss: 1.3092 - acc: 0.6572\n",
            "Epoch 9/10\n",
            "68/68 [==============================] - 325s 5s/step - loss: 1.2632 - acc: 0.6617\n",
            "Epoch 10/10\n",
            "68/68 [==============================] - 328s 5s/step - loss: 1.1980 - acc: 0.6834\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f42700c3050>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_finetuned = model_maker()\n",
        "model_finetuned.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model_finetuned.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLEJrGgVk4Cs",
        "outputId": "f075ebaa-7347-45ec-f451-7b7c4383173e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ],
      "source": [
        "model_finetuned.save('/content/gdrive/MyDrive/Reverse_Image_Search/data/model-finetuned.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JUAC_knshP3",
        "outputId": "043b72a6-fb91-4214-d9cf-c16b715f031b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num images   =  2176\n",
            "Shape of feature_list =  (2176, 101)\n",
            "Time taken in sec =  309.36880373954773\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "feature_list_finetuned = []\n",
        "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
        "\n",
        "feature_list = feature_list_finetuned.reshape(len(feature_list_finetuned), -1)\n",
        "\n",
        "print(\"Num images   = \", len(feature_list_finetuned) )\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_f4qMDpshM1"
      },
      "outputs": [],
      "source": [
        "pickle.dump(feature_list,open('/content/gdrive/MyDrive/Reverse_Image_Search/features-caltech101-resnet-finetuned.pickle', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej3aEaSwshI4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#feature_extraction -Summary \n",
        "- Step1: import Dataset\n",
        "- Step2: Create model_architecture\n",
        "- Step3: Create Extract feature def\n",
        "- Step4: extract freture with REAL dataset and pickle dump\n",
        "- Step5: Let's train finetuned model a and save "
      ],
      "metadata": {
        "id": "L_K0J6ZbcJMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEXT to Reverse Image Search#2 >>> **similarity-search**"
      ],
      "metadata": {
        "id": "fsDnkVCFeE5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OeRbA6SfcP6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "â˜…Reverse Image Search#1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "098b09e1e2314c96961d49beb25600c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105e087b6d654baebef6417f7dd9fba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e2c724cf627472987999eabedbd41f2",
            "max": 8677,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e145bcb2745e40bf984cd53dd9d9879a",
            "value": 8677
          }
        },
        "441f9287bb8243139c85f07b17093d58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f8be57223442afae05614305d5cbae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2c724cf627472987999eabedbd41f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788612e6993b45339eb95788ef506987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a053d37dfc934616990a2652357cc005": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441f9287bb8243139c85f07b17093d58",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_788612e6993b45339eb95788ef506987",
            "value": "100%"
          }
        },
        "a3f59c1620154175a213041bfa5bb0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098b09e1e2314c96961d49beb25600c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a420b14225d54cc7a5656fedf8446bd0",
            "value": " 8677/8677 [1:43:32&lt;00:00,  1.26it/s]"
          }
        },
        "a420b14225d54cc7a5656fedf8446bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8cea24807384a20bf42b870e93833f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a053d37dfc934616990a2652357cc005",
              "IPY_MODEL_105e087b6d654baebef6417f7dd9fba7",
              "IPY_MODEL_a3f59c1620154175a213041bfa5bb0a9"
            ],
            "layout": "IPY_MODEL_57f8be57223442afae05614305d5cbae"
          }
        },
        "e145bcb2745e40bf984cd53dd9d9879a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}